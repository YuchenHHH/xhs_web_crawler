# 并发采集模式使用指南

## 概述

本项目现已支持并发执行多个关键词的采集任务，可以同时启动多个浏览器实例，大幅提升采集效率。

## 主要特性

### 1. 多任务并发
- 支持同时采集多个关键词
- 每个任务使用独立的浏览器实例
- 使用信号量（Semaphore）控制并发数，避免资源耗尽

### 2. 灵活配置
- 任务列表集中配置（`MISSIONS` 列表）
- 支持命令行参数调整并发数
- 独立的输出目录，不同任务互不干扰

### 3. 智能日志
- 每条日志带有 `[关键词]` 前缀
- 实时显示各任务进度
- 执行完成后输出汇总报告

### 4. 错误隔离
- 单个任务失败不影响其他任务
- 独立的异常处理和资源清理
- 支持优雅退出（Ctrl+C）

## 快速开始

### 基本使用

```bash
# 使用默认并发数（3个任务同时运行）
python main.py
```

### 自定义并发数

```bash
# 并发5个任务
python main.py --concurrent 5

# 或使用缩写
python main.py -c 5
```

### 修改任务列表

编辑 `main.py` 中的 `MISSIONS` 列表：

```python
MISSIONS = [
    {"keyword": "番茄炒蛋", "description": "挑选成品菜肴"},
    {"keyword": "红烧肉", "description": "挑选色泽红亮的"},
    {"keyword": "清蒸鱼", "description": "完整鱼身"},
    # 添加更多任务...
]
```

## 配置说明

### 任务参数

在 `main.py` 的 `main()` 函数中可以调整以下参数：

```python
max_notes = 20              # 每轮最多点击的笔记数量
total_rounds = 10           # 总共执行的轮次
browse_images_count = 20    # 每个笔记浏览的图片数量
```

### 并发数建议

| 并发数 | 内存占用 | 适用场景 |
|-------|---------|---------|
| 1-2   | ~2GB    | 低配置机器 |
| 3-5   | ~4GB    | 推荐配置 |
| 6+    | ~6GB+   | 高配置机器 |

**注意**: 并发数过高可能导致：
- 内存不足
- 网络请求被限流
- 浏览器响应缓慢

## 输出结构

每个任务会创建独立的输出目录：

```
output/
├── 番茄炒蛋_20250101_120000/
│   ├── round1_note1_marker123/
│   │   ├── image_001.png
│   │   ├── image_002.png
│   │   └── ...
│   └── ...
├── 红烧肉_20250101_120001/
│   └── ...
└── ...
```

## 执行示例

### 启动输出

```
============================================================
🤖 小红书爬虫 Agent 启动（并发模式）
============================================================

📋 任务列表: 共 5 个关键词
   1. 番茄炒蛋 - 挑选成品菜肴
   2. 红烧肉 - 挑选色泽红亮的
   3. 清蒸鱼 - 完整鱼身
   4. 宫保鸡丁 - 挑选与菜肴相关的内容
   5. 麻婆豆腐 - 挑选与菜肴相关的内容

⚙️  并发配置: 最大并发数 = 3
⚙️  Cookie 文件: ✅ 存在

[番茄炒蛋] 🚀 任务启动
[红烧肉] 🚀 任务启动
[清蒸鱼] 🚀 任务启动
...
```

### 完成报告

```
============================================================
📊 所有任务执行完毕 - 汇总报告
============================================================
⏱️  总耗时: 1234.5 秒

✅ [番茄炒蛋] 成功 - 点击20个 | 详情页15个
✅ [红烧肉] 成功 - 点击20个 | 详情页18个
✅ [清蒸鱼] 成功 - 点击20个 | 详情页12个
✅ [宫保鸡丁] 成功 - 点击20个 | 详情页16个
✅ [麻婆豆腐] 成功 - 点击20个 | 详情页14个

📈 成功: 5/5 | 失败: 0/5
============================================================
```

## 注意事项

### 1. Cookie 共享
所有任务共享同一个 `auth.json` 登录状态。建议在首次运行前确保 Cookie 文件存在且有效。

### 2. 资源占用
- 每个浏览器实例约占用 300-500MB 内存
- 建议根据机器配置调整并发数
- 监控系统资源使用情况

### 3. 网络限流
并发任务过多可能触发平台限流。如遇到大量请求失败，建议：
- 降低并发数
- 增加任务间延迟
- 检查 IP 是否被封禁

### 4. 数据完整性
每个任务独立运行，互不影响。但如果某个任务崩溃，不会重试，需要手动重新运行。

## 常见问题

### Q: 如何只运行部分任务？
A: 修改 `MISSIONS` 列表，注释掉不需要的任务。

### Q: 任务执行顺序是什么？
A: 所有任务几乎同时启动，受信号量限制。前 N 个（N=并发数）会立即执行，后续任务会等待前面的完成。

### Q: 如何暂停/恢复任务？
A: 当前版本不支持暂停/恢复。按 Ctrl+C 会终止所有任务。

### Q: 能否动态添加任务？
A: 当前版本不支持。需要修改代码并重新启动。

## 性能对比

| 模式 | 5个关键词耗时 | 效率提升 |
|-----|-------------|---------|
| 串行模式 | ~100分钟 | - |
| 并发模式（3并发） | ~35分钟 | ~3倍 |
| 并发模式（5并发） | ~20分钟 | ~5倍 |

## 版本历史

- **v2.0** (2025-01-11): 新增并发采集模式
- **v1.0**: 原始串行采集模式

## 技术细节

### 架构设计

```
main()
  ├─ asyncio.Semaphore(3)  # 并发控制
  ├─ run_single_mission()  # 任务1
  │   └─ BrowserManager()  # 独立浏览器
  ├─ run_single_mission()  # 任务2
  │   └─ BrowserManager()
  └─ run_single_mission()  # 任务3
      └─ BrowserManager()
```

### 并发实现

使用 `asyncio.gather()` + `asyncio.Semaphore`:
- `gather()` 启动所有协程
- `Semaphore` 限制同时运行的数量
- `async with semaphore` 自动获取/释放信号量

## 支持

如有问题或建议，请提交 Issue 或 Pull Request。
