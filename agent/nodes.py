"""
Agent èŠ‚ç‚¹å‡½æ•°
å®ç°å…·ä½“çš„ä¸šåŠ¡é€»è¾‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„ä»»åŠ¡
"""
import asyncio
import random
from typing import Dict, List
from playwright.async_api import Page

from agent.state import AgentState
from config.settings import (
    DEFAULT_TIMEOUT,
    SCROLL_PAUSE_TIME,
)
from core.click_verifier import ClickVerifier


async def init_browser_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 1: åˆå§‹åŒ–æµè§ˆå™¨å¹¶è®¿é—®å°çº¢ä¹¦é¦–é¡µ

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 1/3] åˆå§‹åŒ–æµè§ˆå™¨å¹¶è®¿é—®é¦–é¡µ")
    print("="*60)

    browser_manager = state["browser_manager"]
    page = await browser_manager.start()

    # è®¿é—®å°çº¢ä¹¦é¦–é¡µ
    crawler = state["crawler"]
    home_url = crawler.home_url
    print(f"ğŸŒ æ­£åœ¨è®¿é—®: {home_url}")
    await page.goto(home_url, wait_until="domcontentloaded", timeout=30000)

    # ç­‰å¾…é¡µé¢ç¨³å®šï¼ˆæ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸ºï¼‰
    await asyncio.sleep(2)

    print("âœ… é¦–é¡µåŠ è½½å®Œæˆ\n")

    return {
        "page": page,
        "step": "browser_initialized"
    }


async def check_login_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 2: æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰

    æ£€æŸ¥æ–¹æ³•ï¼š
    1. æŸ¥æ‰¾ç™»å½•æŒ‰é’®æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœå­˜åœ¨è¯´æ˜æœªç™»å½•ï¼‰
    2. æŸ¥æ‰¾ç”¨æˆ·å¤´åƒæˆ–ç”¨æˆ·åï¼ˆå¦‚æœå­˜åœ¨è¯´æ˜å·²ç™»å½•ï¼‰

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 2/3] æ£€æŸ¥ç™»å½•çŠ¶æ€")
    print("="*60)

    page: Page = state["page"]
    crawler = state["crawler"]
    is_logged_in = await crawler.detect_login_status(page)

    if is_logged_in:
        print("âœ… æ£€æµ‹åˆ°ç”¨æˆ·ä¿¡æ¯ï¼Œå·²ç™»å½•")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
        print("   å»ºè®®: ç»§ç»­åç»­æ­¥éª¤å‰å…ˆå®Œæˆä¸€æ¬¡æ‰‹åŠ¨ç™»å½•")

    print()
    return {
        "is_logged_in": is_logged_in,
        "step": "login_checked"
    }


async def manual_login_and_save_cookies_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 2.1: æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie

    åœ¨æœªç™»å½•æ—¶æç¤ºç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œè½®è¯¢æ£€æµ‹ç™»å½•æˆåŠŸåè‡ªåŠ¨ä¿å­˜ Cookieã€‚
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 2.1/3] æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie")
    print("="*60)

    page: Page = state["page"]
    browser_manager = state["browser_manager"]
    crawler = state["crawler"]

    print("ğŸ’¡ å½“å‰æœªç™»å½•ï¼Œè¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆç™»å½•ï¼ˆæ‰«ç /å¯†ç å‡å¯ï¼‰")
    print("   ç™»å½•æˆåŠŸåï¼Œæœ¬èŠ‚ç‚¹ä¼šè‡ªåŠ¨ä¿å­˜ Cookieï¼Œåç»­å¯å…æ‰«ç ã€‚")

    max_checks = 8  # æœ€é•¿ç­‰å¾… 8 æ¬¡ï¼Œæ¯æ¬¡ 10 ç§’ï¼Œæ€»è®¡ ~80 ç§’
    wait_interval = 10

    is_logged_in = False

    for i in range(max_checks):
        print(f"   - ç­‰å¾…ç”¨æˆ·ç™»å½•ä¸­... ({i + 1}/{max_checks})")
        await asyncio.sleep(wait_interval)

        is_logged_in = await crawler.detect_login_status(page)
        if is_logged_in:
            print("âœ… æ£€æµ‹åˆ°å·²ç™»å½•ï¼Œæ­£åœ¨ä¿å­˜ Cookie ...")
            await browser_manager.save_cookies()
            break

    if not is_logged_in:
        print("âš ï¸  æœªèƒ½åœ¨è§„å®šæ—¶é—´å†…æ£€æµ‹åˆ°ç™»å½•ï¼Œç»§ç»­åç»­æµç¨‹ï¼ˆå¯èƒ½éœ€è¦å†æ¬¡ç™»å½•ï¼‰")

    return {
        "is_logged_in": is_logged_in,
        "step": "manual_login_completed" if is_logged_in else "manual_login_failed"
    }


async def search_keyword_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 3: æ‰§è¡Œæœç´¢æ“ä½œ

    æµç¨‹ï¼š
    1. å®šä½æœç´¢æ¡†ï¼ˆä½¿ç”¨å¤šç§é€‰æ‹©å™¨ç­–ç•¥ï¼‰
    2. ç‚¹å‡»æœç´¢æ¡†å¹¶è¾“å…¥å…³é”®è¯
    3. æŒ‰å›è½¦æäº¤æœç´¢
    4. ç­‰å¾…æœç´¢ç»“æœåŠ è½½

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 3/3] æ‰§è¡Œæœç´¢æ“ä½œ")
    print("="*60)

    page: Page = state["page"]
    keyword = state["search_keyword"]
    crawler = state["crawler"]

    print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

    try:
        # === æ­¥éª¤ 1: å®šä½æœç´¢æ¡† ===
        print("   - æ­£åœ¨å®šä½æœç´¢æ¡†...")

        # å°è¯•å¤šç§é€‰æ‹©å™¨ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
        search_input = None
        for selector in crawler.search_input_selectors:
            try:
                locator = page.locator(selector).first
                if await locator.is_visible(timeout=2000):
                    search_input = locator
                    print(f"   - âœ… æ‰¾åˆ°æœç´¢æ¡†: {selector}")
                    break
            except:
                continue

        if not search_input:
            raise Exception("æ— æ³•å®šä½æœç´¢æ¡†ï¼Œæ‰€æœ‰é€‰æ‹©å™¨å‡å¤±è´¥")

        # === æ­¥éª¤ 2: ç‚¹å‡»å¹¶è¾“å…¥ ===
        await search_input.wait_for(state="visible", timeout=5000)
        await search_input.click()
        await asyncio.sleep(0.5)  # ç­‰å¾…è¾“å…¥æ¡†æ¿€æ´»

        await search_input.fill(keyword)
        print("   - âœ… è¾“å…¥å®Œæˆ")

        # === æ­¥éª¤ 3: æäº¤æœç´¢ ===
        await page.keyboard.press("Enter")
        print("   - âœ… å›è½¦æäº¤")

        # === æ­¥éª¤ 4: ç­‰å¾…ç»“æœåŠ è½½ ===
        print("   - ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")

        # ç­–ç•¥ 1: ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
        await page.wait_for_load_state("load", timeout=DEFAULT_TIMEOUT)

        # ç­–ç•¥ 2: ç­‰å¾…æœç´¢ç»“æœå®¹å™¨å‡ºç°
        # å°çº¢ä¹¦æœç´¢ç»“æœé€šå¸¸åœ¨ .feeds-container æˆ– .note-item ä¸­
        result_container = page.locator(".feeds-container, .note-item, [class*='search-result']").first
        await result_container.wait_for(state="visible", timeout=DEFAULT_TIMEOUT)

        print("âœ… æœç´¢å®Œæˆï¼Œç»“æœå·²åŠ è½½")

        # ä¸åœ¨æœç´¢åç«‹å³æ»šåŠ¨ï¼Œä¿æŒé¡µé¢åœ¨åˆå§‹çŠ¶æ€
        # await _scroll_once_for_results(page)

        # æˆªå›¾ä¿å­˜ï¼ˆå¯é€‰ï¼Œæ–¹ä¾¿è°ƒè¯•ï¼‰
        # await page.screenshot(path=f"search_result_{keyword}.png")

    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        print("   å¯èƒ½åŸå› :")
        print("   1. æœç´¢æ¡†é€‰æ‹©å™¨å¤±æ•ˆï¼ˆå°çº¢ä¹¦é¡µé¢ç»“æ„å˜åŒ–ï¼‰")
        print("   2. ç½‘ç»œè¶…æ—¶æˆ–é¡µé¢åŠ è½½ç¼“æ…¢")
        print("   3. éœ€è¦ç™»å½•æ‰èƒ½æœç´¢")

        return {
            "step": "search_failed",
        }

    print()
    return {
        "step": "search_completed"
    }


# === æ‰©å±•èŠ‚ç‚¹ ===

async def vision_analysis_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 4: è§†è§‰åˆ†æèŠ‚ç‚¹
    ä½¿ç”¨ GPT-4o Vision åˆ†ææœç´¢ç»“æœé¡µé¢æˆªå›¾å¹¶ç”Ÿæˆæ€»ç»“

    æµç¨‹:
    1. ç­‰å¾…é¡µé¢ç¨³å®šï¼ˆæ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹ï¼‰
    2. æˆªå–å½“å‰å¯è§†åŒºåŸŸ
    3. è°ƒç”¨ GPT-4o è¿›è¡Œè§†è§‰åˆ†æ
    4. æå–ç»“æ„åŒ–æ•°æ®
    5. ç”Ÿæˆé¡µé¢å†…å®¹æ€»ç»“

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 4/4] è§†è§‰åˆ†æä¸å†…å®¹æ€»ç»“")
    print("="*60)

    page: Page = state["page"]

    try:
        # å¯¼å…¥ VisionAnalyzer
        from core.vision_analyzer import VisionAnalyzer

        # === æ­¥éª¤ 1: ç­‰å¾…é¡µé¢ç¨³å®š ===
        print("â³ ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½ç¨³å®š...")
        await asyncio.sleep(2)

        # å¯é€‰: æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
        # await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
        # await asyncio.sleep(1)

        # === æ­¥éª¤ 2: åˆå§‹åŒ–åˆ†æå™¨ ===
        analyzer = VisionAnalyzer()

        # === æ­¥éª¤ 3: æˆªå›¾ + åˆ†æ ===
        result = await analyzer.analyze_page(page)

        # === æ­¥éª¤ 4: æå–æ•°æ® ===
        notes = result.get("notes", [])
        print(f"\nâœ… è§†è§‰åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ° {len(notes)} æ¡ç¬”è®°")

        # æ‰“å°å‰ 3 æ¡ç»“æœé¢„è§ˆ
        if notes:
            print("\nğŸ“‹ å‰ 3 æ¡ç¬”è®°é¢„è§ˆ:")
            for i, note in enumerate(notes[:3], 1):
                print(f"   {i}. {note.get('title', 'N/A')}")
                print(f"      ä½œè€…: {note.get('author', 'N/A')}")
                print(f"      ç‚¹èµ: {note.get('likes', 'N/A')}")
                print(f"      æ ‡ç­¾: {', '.join(note.get('tags', []))}")
                print()

        return {
            "vision_analysis": result,
            "search_results": notes,
            "step": "vision_analysis_completed"
        }

    except ImportError:
        print("âŒ æœªæ‰¾åˆ° VisionAnalyzer æ¨¡å—")
        print("   è¯·ç¡®ä¿å·²å®‰è£… openai ä¾èµ–: pip install openai")
        return {
            "step": "vision_analysis_failed",
            "error_message": "VisionAnalyzer æ¨¡å—æœªæ‰¾åˆ°"
        }

    except ValueError as e:
        print(f"âŒ è§†è§‰åˆ†æå¤±è´¥: {e}")
        print("   å¯èƒ½åŸå› :")
        print("   1. æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("   2. API Key æ— æ•ˆæˆ–é¢åº¦ä¸è¶³")
        return {
            "step": "vision_analysis_failed",
            "error_message": str(e)
        }

    except Exception as e:
        print(f"âŒ è§†è§‰åˆ†æå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return {
            "step": "vision_analysis_failed",
            "error_message": str(e)
        }


async def collect_note_links_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 4: æ”¶é›†ç¬”è®°å¡ç‰‡ä½ç½®ï¼ˆä½¿ç”¨è§†è§‰è¯†åˆ«ï¼‰

    ä½¿ç”¨ GPT-4o Vision è¯†åˆ«å½“å‰è§†å£å†…çš„ç¬”è®°å¡ç‰‡ä½ç½®
    åªè¯†åˆ«ä¸€æ¬¡ï¼Œä¸æ»šåŠ¨ï¼Œå¤„ç†å®Œæ‰€æœ‰ç¬”è®°åå†ç”±å…¶ä»–é€»è¾‘å†³å®šæ˜¯å¦æ»šåŠ¨

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 4/7] ä½¿ç”¨è§†è§‰è¯†åˆ«æ”¶é›†ç¬”è®°ä½ç½®")
    print("="*60)

    page: Page = state["page"]
    max_notes = state.get("max_notes_to_process", 5)

    try:
        from core.vision_locator import VisionLocator

        # åˆå§‹åŒ–è§†è§‰å®šä½å™¨
        vision_locator = VisionLocator()

        print(f"ğŸ¯ ä½¿ç”¨ AI è§†è§‰è¯†åˆ«å½“å‰è§†å£çš„ç¬”è®°ä½ç½®...")

        # ä½¿ç”¨ GPT-4o Vision è¯†åˆ«å½“å‰é¡µé¢çš„ç¬”è®°ä½ç½®
        # è¯†åˆ«å°½å¯èƒ½å¤šçš„ç¬”è®°ï¼Œä½†æœ€å¤šä¸è¶…è¿‡ max_notes
        detected_notes = await vision_locator.locate_note_cards(
            page=page,
            max_notes=max_notes
        )

        if not detected_notes:
            raise Exception("è§†è§‰è¯†åˆ«å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•ç¬”è®°å¡ç‰‡")

        # æ„å»º note_links åˆ—è¡¨
        note_links = []
        for i, note_pos in enumerate(detected_notes):
            note_links.append({
                "index": i,
                "click_x": note_pos["click_x"],
                "click_y": note_pos["click_y"],
                "bounding_box": note_pos.get("bounding_box", {}),
                "title": note_pos.get("title", ""),
                "note_id": note_pos.get("note_id", ""),
            })

        # åªå–å‰ max_notes ä¸ª
        note_links = note_links[:max_notes]

        # è¾“å‡ºç»“æœ
        print(f"\nâœ… æˆåŠŸè¯†åˆ« {len(note_links)} ä¸ªç¬”è®°ä½ç½®")
        for i, link in enumerate(note_links[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
            coord = f"({link['click_x']}, {link['click_y']})"
            title = link.get('title', 'N/A')[:30]
            print(f"   {i + 1}. åæ ‡: {coord} | æ ‡é¢˜: {title}")
        if len(note_links) > 3:
            print(f"   ... è¿˜æœ‰ {len(note_links) - 3} ä¸ª")

        print()
        return {
            "note_links": note_links,
            "current_note_index": 0,
            "step": "note_links_collected"
        }

    except Exception as e:
        print(f"âŒ æ”¶é›†ç¬”è®°ä½ç½®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        return {
            "note_links": [],
            "current_note_index": 0,
            "step": "note_links_collection_failed"
        }


async def process_note_detail_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 5: å¤„ç†å•ä¸ªç¬”è®°è¯¦æƒ…

    ä½¿ç”¨åæ ‡æ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡»ç¬”è®°å¡ç‰‡ï¼Œè¿›å…¥è¯¦æƒ…é¡µï¼Œæˆªå›¾å¹¶æå–æ•°æ®ï¼Œä¿å­˜æ–‡ä»¶

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    page: Page = state["page"]
    crawler = state["crawler"]
    note_links = state.get("note_links", [])
    current_index = state.get("current_note_index", 0)
    output_dir = state.get("output_base_dir", "output")
    processed_notes = state.get("processed_notes", [])
    failed_notes = state.get("failed_notes", [])

    print("\n" + "="*60)
    print(f"ğŸ“ [Node 5/7] å¤„ç†ç¬”è®° {current_index + 1}/{len(note_links)}")
    print("="*60)

    if current_index >= len(note_links):
        print("âš ï¸  æ²¡æœ‰æ›´å¤šç¬”è®°éœ€è¦å¤„ç†")
        return {
            "current_note_index": current_index,
            "step": "no_more_notes"
        }

    current_note = note_links[current_index]
    click_x = current_note.get("click_x", 0)
    click_y = current_note.get("click_y", 0)
    note_id = current_note.get("note_id", "unknown")
    title = current_note.get("title", "N/A")
    bounding_box = current_note.get("bounding_box")

    print(f"ğŸ¯ ç¬”è®°ä¿¡æ¯:")
    print(f"   - æ ‡é¢˜: {title[:50]}")
    print(f"   - ç¬”è®°ID: {note_id}")
    print(f"   - ç‚¹å‡»åæ ‡: ({click_x}, {click_y})")

    # è®°å½•ç‚¹å‡»å‰çš„ URLï¼Œç”¨äºåç»­åˆ¤æ–­æ˜¯å¦çœŸçš„è¿›å…¥äº†è¯¦æƒ…é¡µ
    before_click_url = page.url
    entered_detail = False

    try:
        # === æ­¥éª¤ 1: æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨è§†å£å†… ===
        viewport = page.viewport_size
        viewport_width = viewport["width"]
        viewport_height = viewport["height"]

        print(f"   - è§†å£å°ºå¯¸: {viewport_width}x{viewport_height}")

        if click_y > viewport_height or click_y < 0:
            print(f"   - ğŸ“œ åæ ‡ä¸åœ¨è§†å£å†…ï¼Œæ­£åœ¨æ»šåŠ¨...")
            scroll_amount = click_y - viewport_height // 2
            await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
            await asyncio.sleep(1)
            print("   - âš ï¸  æ»šåŠ¨åä½¿ç”¨åŸåæ ‡ç‚¹å‡»ï¼ˆå¯èƒ½æœ‰åå·®ï¼‰")

        # === æ­¥éª¤ 1.5: åæ ‡éªŒè¯ ===
        verifier = ClickVerifier(selectors=crawler.note_card_selectors)
        verification = await verifier.validate(page, click_x, click_y, bounding_box)

        if not verification["is_valid"]:
            suggestion = (verification["click_x"], verification["click_y"])
            print(f"   - âš ï¸  åæ ‡æœªå‘½ä¸­ç¬”è®°: {verification['reason']}")
            if suggestion != (click_x, click_y):
                print(f"   - ğŸ”„ ä½¿ç”¨å»ºè®®åæ ‡ {suggestion} è¿›è¡Œç‚¹å‡»")
                click_x, click_y = suggestion
            else:
                print("   - âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ç¬”è®°åæ ‡ï¼Œè·³è¿‡è¯¥ç¬”è®°")
                raise Exception("note click verification failed")
        else:
            click_x, click_y = verification["click_x"], verification["click_y"]
            print(f"   - âœ… åæ ‡éªŒè¯é€šè¿‡ï¼Œä½¿ç”¨ ({click_x}, {click_y}) ç‚¹å‡»")

        # === æ­¥éª¤ 1.55: DOM æ ¡å‡†ï¼Œå°†åæ ‡å¸é™„åˆ°æœ€è¿‘å¡ç‰‡ä¸­å¿ƒ ===
        calibrated = await _calibrate_click_with_dom(page, click_x, click_y, crawler.note_card_selectors)
        if calibrated:
            dist = calibrated.get("distance", 0)
            new_x, new_y = calibrated["x"], calibrated["y"]
            if dist > 0:
                print(f"   - ğŸ”„ DOM æ ¡å‡†: è·ç¦»æœ€è¿‘å¡ç‰‡ä¸­å¿ƒ {dist:.1f}pxï¼Œä½¿ç”¨ ({new_x}, {new_y})")
            click_x, click_y = new_x, new_y

        # === æ­¥éª¤ 1.6: åœ¨é¡µé¢ä¸Šæ ‡è®°å³å°†ç‚¹å‡»çš„ä½ç½®ï¼ˆä»…è°ƒè¯•å¯è§†åŒ–ï¼‰ ===
        await _show_click_marker(page, click_x, click_y)
        await asyncio.sleep(1)  # è®©æ ‡è®°æ˜¾ç¤º 1 ç§’å†ç‚¹å‡»

        # === æ­¥éª¤ 2: æ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡» ===
        print(f"   - ğŸ–±ï¸  æ¨¡æ‹Ÿé¼ æ ‡ç‚¹å‡»åæ ‡ ({click_x}, {click_y})ï¼ˆå…ˆç§»åŠ¨/æ‚¬åœå†ç‚¹å‡»ï¼‰...")
        await _human_like_click(page, click_x, click_y)
        print("   - âœ… ç‚¹å‡»å®Œæˆï¼Œç­‰å¾…é¡µé¢è·³è½¬...")

        # ç­‰å¾…é¡µé¢å¯¼èˆªå®Œæˆ
        await page.wait_for_load_state("domcontentloaded", timeout=15000)

        # ç‚¹å‡»åæ ¡éªŒæ˜¯å¦çœŸçš„è¿›å…¥äº†ç¬”è®°è¯¦æƒ…é¡µ
        verification = await _verify_detail_page_entry(page, before_click_url, crawler.note_detail_selectors)
        entered_detail = verification.get("entered", False)
        if not entered_detail:
            reason = verification.get("reason", "unknown")
            print(f"   - âš ï¸  æœªæ£€æµ‹åˆ°è¯¦æƒ…é¡µï¼Œè·³è¿‡è¯¥åæ ‡ã€‚åŸå› : {reason}")
            failed_notes.append({
                "index": current_index,
                "note_id": note_id,
                "click_position": {"x": click_x, "y": click_y},
                "error": f"did not enter detail page: {reason}"
            })
            return {
                "processed_notes": processed_notes,
                "failed_notes": failed_notes,
                "current_note_index": current_index + 1,
                "step": "note_click_invalid",
                "last_click_entered_detail": False
            }

        print(f"   - âœ… å·²è¿›å…¥è¯¦æƒ…é¡µï¼ˆ{verification.get('via', 'detected via markers')}ï¼‰")

        # ç­‰å¾…è¯¦æƒ…é¡µå†…å®¹åŠ è½½
        title_selector = crawler.note_detail_selectors["title"]

        try:
            await page.locator(title_selector).first.wait_for(state="visible", timeout=10000)
        except:
            print("   - âš ï¸  è¯¦æƒ…é¡µåŠ è½½å¯èƒ½ä¸å®Œæ•´ï¼Œç»§ç»­å°è¯•...")

        # é¢å¤–ç­‰å¾…ç¡®ä¿é¡µé¢ç¨³å®š
        await asyncio.sleep(1.5)

        print("   - âœ… è¯¦æƒ…é¡µåŠ è½½å®Œæˆ")

        # === æ­¥éª¤ 2: æˆªå›¾ ===
        from core.file_manager import FileManager

        screenshot_path = FileManager.get_note_filename(output_dir, current_index, "png")
        screenshot_success = await FileManager.save_screenshot(page, screenshot_path)

        # === æ­¥éª¤ 3: æå–æ•°æ® ===
        from core.note_extractor import NoteDetailExtractor

        print("   - ğŸ“ æ­£åœ¨æå–ç¬”è®°æ•°æ®...")
        extractor = NoteDetailExtractor()
        # note_data = await extractor.extract_from_page(page)
        note_data = {}
        print(f"   - âœ… æå–å®Œæˆ: {note_data.get('title', 'N/A')[:50]}")

        # === æ­¥éª¤ 4: ä¿å­˜ JSON ===
        json_path = FileManager.get_note_filename(output_dir, current_index, "json")

        # åˆå¹¶å…ƒæ•°æ®åˆ°æ•°æ®ä¸­
        note_data["url"] = page.url  # ä½¿ç”¨å½“å‰é¡µé¢URLï¼ˆè¯¦æƒ…é¡µURLï¼‰
        note_data["index"] = current_index
        note_data["click_position"] = {"x": click_x, "y": click_y}
        note_data["note_id"] = note_id

        json_success = FileManager.save_json(note_data, json_path)

        # === æ­¥éª¤ 5: è®°å½•ç»“æœ ===
        processed_notes.append({
            "index": current_index,
            "url": page.url,  # ä½¿ç”¨å®é™…çš„è¯¦æƒ…é¡µURL
            "data": note_data,
            "screenshot_path": screenshot_path,
            "json_path": json_path,
            "success": screenshot_success and json_success
        })

        print(f"âœ… ç¬”è®° {current_index + 1} å¤„ç†å®Œæˆ\n")

        return {
            "processed_notes": processed_notes,
            "current_note_index": current_index + 1,
            "step": "note_processed",
            "last_click_entered_detail": True
        }

    except Exception as e:
        print(f"âŒ å¤„ç†ç¬”è®° {current_index + 1} å¤±è´¥: {e}")

        # è®°å½•å¤±è´¥
        failed_notes.append({
            "index": current_index,
            "note_id": note_id,
            "click_position": {"x": click_x, "y": click_y},
            "error": str(e)
        })

        print(f"   - è·³è¿‡æ­¤ç¬”è®°ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª\n")

        return {
            "processed_notes": processed_notes,
            "failed_notes": failed_notes,
            "current_note_index": current_index + 1,
            "step": "note_processing_failed_but_continuing",
            "last_click_entered_detail": entered_detail
        }


async def navigate_back_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 6: è¿”å›æœç´¢ç»“æœé¡µ

    ä½¿ç”¨æµè§ˆå™¨åé€€æŒ‰é’®è¿”å›åˆ°æœç´¢ç»“æœé¡µ

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 6/7] è¿”å›æœç´¢ç»“æœé¡µ")
    print("="*60)

    page: Page = state["page"]
    last_entered_detail = state.get("last_click_entered_detail", True)

    if last_entered_detail is False:
        print("   - â­ï¸ ä¸Šä¸€æ¬¡ç‚¹å‡»æœªè¿›å…¥è¯¦æƒ…é¡µï¼Œä¿æŒåœ¨æœç´¢ç»“æœé¡µï¼Œè·³è¿‡åé€€æ“ä½œ\n")
        return {
            "step": "skipped_back_no_detail",
            "last_click_entered_detail": None
        }

    try:
        print("   - ğŸ”™ æ‰§è¡Œåé€€æ“ä½œ...")

        # ä½¿ç”¨æµè§ˆå™¨åé€€
        await page.go_back(wait_until="domcontentloaded", timeout=10000)

        # ç­‰å¾…æœç´¢ç»“æœå®¹å™¨é‡æ–°å‡ºç°
        await asyncio.sleep(1)

        # éªŒè¯æ˜¯å¦æˆåŠŸè¿”å›
        result_container = page.locator(".feeds-container, .note-item, [class*='search-result']").first
        await result_container.wait_for(state="visible", timeout=5000)

        print("   - âœ… æˆåŠŸè¿”å›æœç´¢ç»“æœé¡µ\n")

        return {
            "step": "navigated_back"
        }

    except Exception as e:
        print(f"âš ï¸  åé€€æ“ä½œå¤±è´¥: {e}")
        print("   - å°è¯•å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥å¯¼èˆªåˆ°æœç´¢é¡µ...")

        try:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šé‡æ–°æ‰§è¡Œæœç´¢
            keyword = state.get("search_keyword", "")
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
            await page.goto(search_url, wait_until="domcontentloaded", timeout=10000)
            await asyncio.sleep(1)

            print("   - âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæˆåŠŸ\n")
            return {
                "step": "navigated_back_fallback"
            }
        except Exception as e2:
            print(f"âŒ å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}\n")
            return {
                "step": "navigation_back_failed",
                "error_message": str(e2)
            }


async def scroll_for_more_notes_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 6.5: å‘ä¸‹æ»šåŠ¨ä»¥åŠ è½½æ–°ä¸€æ‰¹ç¬”è®°
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 6.5/7] æ»šåŠ¨åŠ è½½æ›´å¤šç¬”è®°")
    print("="*60)

    page: Page = state["page"]

    try:
        print(f"   - âŒ„ æ¨¡æ‹Ÿå³ä¾§æŠ½å±‰æ»šåŠ¨ï¼ŒæŒç»­ {SCROLL_PAUSE_TIME} ç§’åŠ è½½æ›´å¤šå†…å®¹")
        await scroll_right_modal(page, duration=SCROLL_PAUSE_TIME)
        await asyncio.sleep(0.5)  # é¢å¤–ç­‰å¾…åŠç§’ä»¥ä¾¿é¡µé¢æ¸²æŸ“

        # ç¡®è®¤ä»åœ¨æœç´¢ç»“æœé¡µ
        result_container = page.locator(".feeds-container, .note-item, [class*='search-result']").first
        await result_container.wait_for(state="visible", timeout=5000)

        print("   - âœ… æ»šåŠ¨å®Œæˆï¼Œå‡†å¤‡é‡æ–°æ”¶é›†ç¬”è®°ä½ç½®\n")
        return {
            "step": "scrolled_for_more"
        }

    except Exception as e:
        print(f"âŒ æ»šåŠ¨åŠ è½½å¤±è´¥: {e}\n")
        return {
            "step": "scroll_failed",
            "error_message": str(e)
        }


async def finalize_extraction_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ 7: å®Œæˆæå–æµç¨‹

    ä¿å­˜å…ƒæ•°æ®å¹¶è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [Node 7/7] å®Œæˆæå–æµç¨‹")
    print("="*60)

    output_dir = state.get("output_base_dir", "")
    processed_notes = state.get("processed_notes", [])
    failed_notes = state.get("failed_notes", [])
    keyword = state.get("search_keyword", "")

    try:
        from core.file_manager import FileManager
        from datetime import datetime

        # ä¿å­˜ä¼šè¯å…ƒæ•°æ®
        metadata = {
            "keyword": keyword,
            "total_notes_found": len(processed_notes) + len(failed_notes),
            "successfully_processed": len(processed_notes),
            "failed": len(failed_notes),
            "created_at": datetime.now().isoformat(),
            "output_directory": output_dir
        }

        FileManager.save_metadata(output_dir, metadata)

        # è¾“å‡ºç»Ÿè®¡
        print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
        print(f"   - æœç´¢å…³é”®è¯: {keyword}")
        print(f"   - æ‰¾åˆ°ç¬”è®°æ•°: {metadata['total_notes_found']}")
        print(f"   - æˆåŠŸå¤„ç†: {metadata['successfully_processed']}")
        print(f"   - å¤±è´¥è·³è¿‡: {metadata['failed']}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")

        if processed_notes:
            print(f"\nâœ… æˆåŠŸå¤„ç†çš„ç¬”è®°:")
            for note in processed_notes:
                title = note["data"].get("title", "N/A")[:40]
                print(f"   {note['index'] + 1}. {title}")

        if failed_notes:
            print(f"\nâš ï¸  å¤„ç†å¤±è´¥çš„ç¬”è®°:")
            for fail in failed_notes:
                print(f"   {fail['index'] + 1}. {fail.get('error', 'Unknown error')[:50]}")

        print()

        return {
            "step": "extraction_completed"
        }

    except Exception as e:
        print(f"âŒ å®Œæˆæµç¨‹æ—¶å‡ºé”™: {e}\n")
        return {
            "step": "finalization_failed",
            "error_message": str(e)
        }


async def download_images_node(state: AgentState) -> dict:
    """
    èŠ‚ç‚¹ Y: å›¾ç‰‡ä¸‹è½½èŠ‚ç‚¹ï¼ˆé¢„ç•™ï¼‰
    ç”¨äºåç»­å®ç°æœç´¢ç»“æœå›¾ç‰‡ä¸‹è½½åŠŸèƒ½

    Args:
        state: Agent çŠ¶æ€

    Returns:
        æ›´æ–°åçš„çŠ¶æ€å­—å…¸
    """
    print("\n" + "="*60)
    print("ğŸ“ [æ‰©å±•èŠ‚ç‚¹] å›¾ç‰‡ä¸‹è½½")
    print("="*60)
    print("âš ï¸  è¯¥èŠ‚ç‚¹å°šæœªå®ç°")
    print()

    return {
        "step": "download_completed"
    }


# === è¾…åŠ©å‡½æ•° ===
# æ³¨æ„: _detect_login_status å‡½æ•°å·²ç§»è‡³ core/crawler_strategy.py ä¸­çš„ CrawlerStrategy.detect_login_status()

async def scroll_right_modal(page: Page, duration: float = 1.0):
    """
    é’ˆå¯¹å³ä¾§æµ®å±‚/æŠ½å±‰çš„æ»šåŠ¨ï¼ˆç±»ä¼¼å°çº¢ä¹¦/Instagram è¯¦æƒ…é¡µï¼‰
    å…ˆå°†é¼ æ ‡ç§»åŠ¨åˆ°å³ä¾§ä¸­éƒ¨ï¼Œå†ç”¨é¼ æ ‡æ»šè½®æ¨¡æ‹Ÿäººç±»æ»šåŠ¨ã€‚
    """
    try:
        viewport = page.viewport_size or {"width": 1280, "height": 800}
        target_x = int(viewport["width"] * 0.75)
        target_y = int(viewport["height"] * 0.5)
    except Exception:
        target_x, target_y = 1000, 500

    # å¯è§†åŒ–æ ‡è®°æ»šåŠ¨ç›®æ ‡
    await _show_click_marker(page, target_x, target_y)

    # ç§»åŠ¨é¼ æ ‡åˆ°ç›®æ ‡åŒºåŸŸï¼Œç¡®ä¿æ»šè½®äº‹ä»¶è½åœ¨æµ®å±‚ä¸Š
    await page.mouse.move(target_x, target_y)

    start = asyncio.get_event_loop().time()
    while asyncio.get_event_loop().time() - start < duration:
        delta_y = random.randint(100, 300)
        await page.mouse.wheel(0, delta_y)
        await asyncio.sleep(random.uniform(0.05, 0.1))


async def _show_click_marker(page: Page, x: int, y: int):
    """
    åœ¨é¡µé¢ä¸Šæ˜¾ç¤ºä¸€ä¸ªä¸´æ—¶æ ‡è®°ï¼ŒæŒ‡ç¤ºå³å°†ç‚¹å‡»çš„ä½ç½®ï¼ˆè°ƒè¯•ç”¨ï¼‰
    """
    await page.evaluate(
        """({ x, y }) => {
            const existing = document.getElementById('codex-click-marker');
            if (existing) existing.remove();

            const marker = document.createElement('div');
            marker.id = 'codex-click-marker';
            marker.style.position = 'fixed';
            marker.style.left = `${x}px`;
            marker.style.top = `${y}px`;
            marker.style.transform = 'translate(-50%, -50%)';
            marker.style.width = '24px';
            marker.style.height = '24px';
            marker.style.border = '2px solid red';
            marker.style.borderRadius = '50%';
            marker.style.boxShadow = '0 0 6px rgba(255,0,0,0.7)';
            marker.style.pointerEvents = 'none';
            marker.style.zIndex = 999999;

            const cross = document.createElement('div');
            cross.style.position = 'absolute';
            cross.style.left = '50%';
            cross.style.top = '50%';
            cross.style.width = '16px';
            cross.style.height = '16px';
            cross.style.transform = 'translate(-50%, -50%)';
            cross.style.pointerEvents = 'none';
            cross.style.background = 'linear-gradient(transparent 45%, red 45%, red 55%, transparent 55%), linear-gradient(90deg, transparent 45%, red 45%, red 55%, transparent 55%)';
            marker.appendChild(cross);

            document.body.appendChild(marker);

            // è‡ªåŠ¨ç§»é™¤ï¼Œé¿å…å¹²æ‰°åç»­æ“ä½œ
            setTimeout(() => marker.remove(), 1500);
        }""",
        {"x": x, "y": y},
    )


async def _scroll_once_for_results(page: Page):
    """
    æœç´¢å®Œæˆåæ»šåŠ¨ä¸€æ¬¡ï¼Œä¿ƒå‘æ‡’åŠ è½½
    """
    try:
        viewport = page.viewport_size or {"width": 1280, "height": 800}
        target_x = int(viewport["width"] * 0.5)
        target_y = int(viewport["height"] * 0.5)
    except Exception:
        target_x, target_y = 800, 500

    await page.mouse.move(target_x, target_y)
    delta_y = random.randint(200, 400)
    await page.mouse.wheel(0, delta_y)
    await asyncio.sleep(0.2)


async def _human_like_click(page: Page, x: int, y: int):
    """
    ç±»äººç‚¹å‡»ï¼šå…ˆç§»åŠ¨é¼ æ ‡åˆ°ç›®æ ‡ç‚¹ï¼Œæ‚¬åœ/åœé¡¿åå†ç‚¹å‡»
    """
    jitter_x = x + random.randint(-3, 3)
    jitter_y = y + random.randint(-3, 3)

    # ç¼“æ…¢ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼ˆåˆ†ä¸¤æ®µï¼Œæ¨¡æ‹Ÿäººæ‰‹ç§»åŠ¨ï¼‰
    mid_x = (x + jitter_x) // 2
    mid_y = (y + jitter_y) // 2
    await page.mouse.move(mid_x, mid_y, steps=8)
    await page.mouse.move(jitter_x, jitter_y, steps=6)

    # æ‚¬åœ/åœé¡¿
    await asyncio.sleep(random.uniform(0.12, 0.25))

    # ç‚¹å‡»
    await page.mouse.click(jitter_x, jitter_y)
    await asyncio.sleep(random.uniform(0.05, 0.12))


async def _verify_detail_page_entry(
    page: Page,
    before_url: str,
    note_detail_selectors: Dict[str, str],
    timeout: int = 8000
) -> Dict:
    """
    åˆ¤æ–­æ˜¯å¦ä»æœç´¢é¡µè¿›å…¥äº†ç¬”è®°è¯¦æƒ…é¡µ

    Args:
        page: Playwright Page
        before_url: ç‚¹å‡»å‰çš„ URL
        note_detail_selectors: è¯¦æƒ…é¡µé€‰æ‹©å™¨å­—å…¸ï¼ˆä» crawler strategy ä¼ å…¥ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    URL åˆ¤æ–­è§„åˆ™ï¼š
    - è¯¦æƒ…é¡µ: https://www.xiaohongshu.com/explore/66c32e22000000001f01fd24?...
    - æœç´¢é¡µ(æœªå˜): https://www.xiaohongshu.com/search_result?keyword=...
    - æœç´¢é¡µ(ç›¸å…³æœç´¢): https://www.xiaohongshu.com/search_result?keyword=...ï¼ˆkeywordå˜äº†ï¼‰
    """
    detail_url_keywords = ["/explore/", "/discovery/item/"]
    search_url_keywords = ["/search_result"]
    selectors = list(note_detail_selectors.values())
    deadline = asyncio.get_event_loop().time() + timeout / 1000

    while asyncio.get_event_loop().time() < deadline:
        current_url = page.url

        # 1. ä¼˜å…ˆæ£€æŸ¥ï¼šå¦‚æœåŒ…å«è¯¦æƒ…é¡µè·¯å¾„ï¼Œç«‹å³è¿”å›æˆåŠŸ
        if any(k in current_url for k in detail_url_keywords):
            return {"entered": True, "via": "url_explore_path", "url": current_url}

        # 2. å¦‚æœä»åœ¨æœç´¢é¡µï¼Œè¯´æ˜ç‚¹å‡»æ— æ•ˆï¼ˆç‚¹åˆ°ç©ºç™½æˆ–"å¤§å®¶éƒ½æƒ³æœ"ï¼‰
        if any(k in current_url for k in search_url_keywords):
            # å³ä½¿ URL ç¨æœ‰å˜åŒ–ï¼ˆå¦‚ keyword å˜äº†ï¼‰ï¼Œä»ç„¶æ˜¯æœç´¢é¡µ
            return {
                "entered": False,
                "reason": "still_on_search_page",
                "url": current_url
            }

        # 3. é€šè¿‡è¯¦æƒ…é¡µç‰¹å¾å…ƒç´ æ£€æµ‹ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        for sel in selectors:
            try:
                locator = page.locator(sel).first
                if await locator.is_visible(timeout=500):
                    return {"entered": True, "via": f"selector:{sel}", "url": current_url}
            except Exception:
                continue

        await asyncio.sleep(0.3)

    return {
        "entered": False,
        "reason": "detail markers not found (timeout)",
        "url": page.url
    }


async def _calibrate_click_with_dom(page: Page, x: int, y: int, note_card_selectors: List[str]):
    """
    æ ¹æ® DOM å®šä½æœ€è¿‘çš„ note å¡ç‰‡ä¸­å¿ƒç‚¹ï¼Œç”¨äºçº å Vision åæ ‡

    Args:
        page: Playwright Page
        x: ç›®æ ‡ x åæ ‡
        y: ç›®æ ‡ y åæ ‡
        note_card_selectors: ç¬”è®°å¡ç‰‡é€‰æ‹©å™¨åˆ—è¡¨ï¼ˆä» crawler strategy ä¼ å…¥ï¼‰
    """
    try:
        selector_str = ",".join(note_card_selectors)
        excluded_words = ["ç›¸å…³æœç´¢", "å¤§å®¶éƒ½åœ¨æœ", "çŒœä½ æƒ³æœ", "çƒ­é—¨æœç´¢"]
        nearest = await page.evaluate(
            """({ x, y, selectorStr, excludedWords }) => {
                const nodes = Array.from(document.querySelectorAll(selectorStr));
                let best = null;
                let bestDist = Number.POSITIVE_INFINITY;
                for (const el of nodes) {
                    const text = (el.innerText || "").slice(0, 50);
                    if (text && excludedWords.some(w => text.includes(w))) continue;
                    const rect = el.getBoundingClientRect();
                    const cx = rect.x + rect.width / 2;
                    const cy = rect.y + rect.height / 2;
                    const dx = cx - x;
                    const dy = cy - y;
                    const dist = Math.hypot(dx, dy);
                    if (dist < bestDist) {
                        bestDist = dist;
                        best = { x: cx, y: cy, width: rect.width, height: rect.height };
                    }
                }
                if (!best) return null;
                return { ...best, distance: bestDist };
            }""",
            {"x": x, "y": y, "selectorStr": selector_str, "excludedWords": excluded_words},
        )
        return nearest
    except Exception as e:
        print(f"   - âš ï¸  DOM æ ¡å‡†å¤±è´¥: {e}")
        return None
