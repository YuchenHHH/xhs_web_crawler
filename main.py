"""
å¤šå¹³å°çˆ¬è™« Agent - ä¸»å…¥å£
æ”¯æŒå°çº¢ä¹¦ã€Pinterestç­‰å¤šä¸ªå¹³å°
æ”¯æŒå¹¶å‘æ‰§è¡Œå¤šä¸ªå…³é”®è¯é‡‡é›†ä»»åŠ¡
"""
import asyncio
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = Path(__file__).parent / ".env"
if env_path.exists():
    load_dotenv(env_path)

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.browser_manager import BrowserManager
from config.sites.xhs import XHSCrawlerStrategy
from config.sites.pinterest import PinterestCrawlerStrategy
from agent.graph import run_click_graph
from agent.nodes import (
    init_browser_node,
    check_login_node,
    manual_login_and_save_cookies_node,
    search_keyword_node,
)
from utils.logger import get_logger

# åˆå§‹åŒ–æ—¥å¿—å™¨
logger = get_logger()
logger.info(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}" if env_path.exists() else f"âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")


# ============================================
# ä»»åŠ¡é…ç½®åˆ—è¡¨
# ============================================
MISSIONS = [
    {"site": "xiaohongshu", "keyword": "å®«ä¿é¸¡ä¸åšæ³•", "description": "åªæŒ‘é€‰å’Œèœè°±,åšèœæµç¨‹ç›¸å…³çš„å†…å®¹"},
]


def get_crawler_strategy(site: str):
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®ç«™ç‚¹åç§°è·å–å¯¹åº”çš„çˆ¬è™«ç­–ç•¥

    Args:
        site: ç«™ç‚¹æ ‡è¯†ï¼ˆxiaohongshu, pinterestç­‰ï¼‰

    Returns:
        å¯¹åº”çš„ CrawlerStrategy å®ä¾‹

    Raises:
        ValueError: ä¸æ”¯æŒçš„ç«™ç‚¹ç±»å‹
    """
    strategies = {
        "xiaohongshu": XHSCrawlerStrategy,
        "pinterest": PinterestCrawlerStrategy,
    }

    strategy_class = strategies.get(site)
    if not strategy_class:
        raise ValueError(f"ä¸æ”¯æŒçš„ç«™ç‚¹: {site}ã€‚å¯ç”¨ç«™ç‚¹: {list(strategies.keys())}")

    return strategy_class()


async def run_single_mission(
    semaphore: asyncio.Semaphore,
    mission_config: Dict[str, str],
    max_notes: int = 20,
    total_rounds: int = 10,
    browse_images_count: int = 20,
    max_images: int = None
) -> Dict:
    """
    æ‰§è¡Œå•ä¸ªå…³é”®è¯çš„é‡‡é›†ä»»åŠ¡ï¼ˆç‹¬ç«‹æµè§ˆå™¨å®ä¾‹ï¼‰

    Args:
        semaphore: ä¿¡å·é‡ï¼Œç”¨äºæ§åˆ¶å¹¶å‘æ•°
        mission_config: ä»»åŠ¡é…ç½® {"site": "...", "keyword": "...", "description": "..."}
        max_notes: æ¯è½®æœ€å¤šç‚¹å‡»çš„ç¬”è®°æ•°é‡
        total_rounds: æ€»å…±æ‰§è¡Œçš„è½®æ¬¡
        browse_images_count: æ¯ä¸ªç¬”è®°è¿›å…¥è¯¦æƒ…é¡µåæŒ‰å³é”®æµè§ˆå›¾ç‰‡çš„æ¬¡æ•°
        max_images: å›¾ç‰‡æ€»æ•°é™åˆ¶ï¼Œè¾¾åˆ°åè‡ªåŠ¨ç»“æŸä»»åŠ¡ï¼ˆNone=ä¸é™åˆ¶ï¼‰

    Returns:
        ä»»åŠ¡æ‰§è¡Œç»“æœæ‘˜è¦
    """
    keyword = mission_config["keyword"]
    description = mission_config["description"]
    site = mission_config.get("site", "xiaohongshu")  # é»˜è®¤ä¸ºå°çº¢ä¹¦ï¼ˆå‘åå…¼å®¹ï¼‰

    async with semaphore:
        # åˆå§‹åŒ–ç«™ç‚¹ç‰¹å®šçš„ BrowserManager
        browser_manager = BrowserManager(site=site)

        # ä½¿ç”¨å·¥å‚å‡½æ•°è·å–ç«™ç‚¹ç‰¹å®šçš„çˆ¬è™«ç­–ç•¥
        crawler = get_crawler_strategy(site)

        try:
            logger.info(f"\n[{site.upper()} | {keyword}] ğŸš€ ä»»åŠ¡å¯åŠ¨")

            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåŒ…å«ç«™ç‚¹å‰ç¼€ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(__file__).parent / "output" / f"{site}_{keyword}_{timestamp}"
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"[{site.upper()} | {keyword}] ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

            state = {
                "browser_manager": browser_manager,
                "crawler": crawler,
                "page": None,
                "search_keyword": keyword,
                "search_description": description,
                "max_notes_to_process": max_notes,
                "current_note_index": 0,
                "note_links": [],
                "processed_notes": [],
                "failed_notes": [],
                "output_base_dir": str(output_dir),
                "step": "not_started",
                "is_logged_in": False,
            }

            config_msg = f"[{site.upper()} | {keyword}] ğŸ“‹ é…ç½®: æ¯è½®{max_notes}ä¸ªç¬”è®° | {total_rounds}è½® | æ¯ç¬”è®°æµè§ˆ{browse_images_count}å¼ å›¾"
            if max_images:
                config_msg += f" | å›¾ç‰‡æ€»æ•°é™åˆ¶{max_images}å¼ "
            logger.info(config_msg)

            # åˆå§‹åŒ–æµè§ˆå™¨
            updates = await init_browser_node(state)
            state.update(updates)

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            updates = await check_login_node(state)
            state.update(updates)

            # å¦‚æœæœªç™»å½•ï¼Œè§¦å‘æ‰‹åŠ¨ç™»å½•æµç¨‹
            if not state.get("is_logged_in"):
                logger.warning(f"[{site.upper()} | {keyword}] âš ï¸  æœªç™»å½•ï¼Œå¯åŠ¨æ‰‹åŠ¨ç™»å½•æµç¨‹...")
                logger.info(f"[{site.upper()} | {keyword}] ğŸ’¡ è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•æ“ä½œ")
                logger.info(f"[{site.upper()} | {keyword}] ğŸ“ ç™»å½•æˆåŠŸåå°†è‡ªåŠ¨ä¿å­˜åˆ°: auth/{site}.json")

                # æ‰§è¡Œæ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜ Cookie
                updates = await manual_login_and_save_cookies_node(state)
                state.update(updates)

                # æ£€æŸ¥ç™»å½•æ˜¯å¦æˆåŠŸ
                if not state.get("is_logged_in"):
                    logger.error(f"[{site.upper()} | {keyword}] âŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢")
                    return {
                        "site": site,
                        "keyword": keyword,
                        "status": "failed",
                        "error": "ç™»å½•å¤±è´¥æˆ–è¶…æ—¶"
                    }
                else:
                    logger.info(f"[{site.upper()} | {keyword}] âœ… ç™»å½•æˆåŠŸï¼ŒCookieå·²ä¿å­˜")

            # æœç´¢å…³é”®è¯
            updates = await search_keyword_node(state)
            state.update(updates)

            # æ‰§è¡Œç‚¹å‡»ä»»åŠ¡
            logger.info(f"[{site.upper()} | {keyword}] ğŸ¯ å¼€å§‹æ‰§è¡Œç‚¹å‡»ä»»åŠ¡...")
            click_result = await run_click_graph(
                page=state["page"],
                crawler=crawler,
                max_notes=max_notes,
                total_rounds=total_rounds,
                browse_images_arrow_count=browse_images_count,
                content_description=description,
                output_dir=str(output_dir),
                max_images=max_images,
            )

            # ç»Ÿè®¡ç»“æœ
            entered_count = len(
                [c for c in click_result.get("clicked", []) if c.get("entered_detail")]
            )

            result_summary = {
                "site": site,
                "keyword": keyword,
                "status": "success",
                "rounds": click_result.get('current_round', 1) - 1,
                "total_clicked": len(click_result.get('clicked', [])),
                "total_failed": len(click_result.get('failures', [])),
                "entered_detail": entered_count,
                "output_dir": str(output_dir),
            }

            logger.info(f"\n[{site.upper()} | {keyword}] âœ… ä»»åŠ¡å®Œæˆ - ç‚¹å‡»{result_summary['total_clicked']}ä¸ª | è¿›å…¥è¯¦æƒ…é¡µ{entered_count}ä¸ª")

            return result_summary

        except KeyboardInterrupt:
            logger.warning(f"\n[{site.upper()} | {keyword}] âš ï¸  ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
            return {"site": site, "keyword": keyword, "status": "interrupted", "error": "ç”¨æˆ·ä¸­æ–­"}

        except Exception as e:
            logger.error(f"\n[{site.upper()} | {keyword}] âŒ ä»»åŠ¡å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return {"site": site, "keyword": keyword, "status": "failed", "error": str(e)}

        finally:
            # æ¸…ç†èµ„æº
            logger.info(f"[{site.upper()} | {keyword}] ğŸ§¹ æ¸…ç†æµè§ˆå™¨èµ„æº...")
            await browser_manager.close()


async def main(max_concurrent: int = 3):
    """
    å¹¶å‘æ‰§è¡Œå¤šä¸ªé‡‡é›†ä»»åŠ¡

    Args:
        max_concurrent: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆé»˜è®¤3ä¸ªï¼‰
    """
    logger.info("\n" + "="*60)
    logger.info("ğŸ¤– å¤šå¹³å°çˆ¬è™« Agent å¯åŠ¨ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
    logger.info("="*60 + "\n")

    logger.info(f"ğŸ“‹ ä»»åŠ¡åˆ—è¡¨: å…± {len(MISSIONS)} ä¸ªä»»åŠ¡")
    for i, mission in enumerate(MISSIONS, 1):
        site = mission.get("site", "xiaohongshu")
        logger.info(f"   {i}. [{site.upper()}] {mission['keyword']} - {mission['description']}")

    logger.info(f"\nâš™ï¸  å¹¶å‘é…ç½®: æœ€å¤§å¹¶å‘æ•° = {max_concurrent}")
    logger.info(f"âš™ï¸  è®¤è¯ç›®å½•: auth/ (å„å¹³å°ä½¿ç”¨ç‹¬ç«‹è®¤è¯æ–‡ä»¶)")
    logger.info("")

    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(max_concurrent)

    # ä»»åŠ¡å‚æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    max_notes = 20
    total_rounds = 10
    browse_images_count = 20
    max_images = 50  # æ¯ä¸ªå…³é”®è¯çš„å›¾ç‰‡æ€»æ•°é™åˆ¶ï¼ˆè®¾ä¸º None åˆ™ä¸é™åˆ¶ï¼‰

    try:
        # å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
        start_time = datetime.now()
        tasks = [
            run_single_mission(
                semaphore=semaphore,
                mission_config=mission,
                max_notes=max_notes,
                total_rounds=total_rounds,
                browse_images_count=browse_images_count,
                max_images=max_images
            )
            for mission in MISSIONS
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = datetime.now()

        # æ‰“å°æ±‡æ€»ç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯• - æ±‡æ€»æŠ¥å‘Š")
        logger.info("="*60)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {(end_time - start_time).total_seconds():.1f} ç§’")
        logger.info("")

        success_count = 0
        failed_count = 0

        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
                failed_count += 1
            elif isinstance(result, dict):
                status = result.get('status', 'unknown')
                site = result.get('site', 'unknown')
                keyword = result.get('keyword', 'N/A')

                if status == 'success':
                    logger.info(f"âœ… [{site.upper()} | {keyword}] æˆåŠŸ - ç‚¹å‡»{result.get('total_clicked', 0)}ä¸ª | è¯¦æƒ…é¡µ{result.get('entered_detail', 0)}ä¸ª")
                    success_count += 1
                elif status == 'interrupted':
                    logger.warning(f"âš ï¸  [{site.upper()} | {keyword}] ä¸­æ–­")
                    failed_count += 1
                else:
                    logger.error(f"âŒ [{site.upper()} | {keyword}] å¤±è´¥ - {result.get('error', 'Unknown error')}")
                    failed_count += 1

        logger.info("")
        logger.info(f"ğŸ“ˆ æˆåŠŸ: {success_count}/{len(MISSIONS)} | å¤±è´¥: {failed_count}/{len(MISSIONS)}")
        logger.info("="*60 + "\n")

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼ˆæ‰€æœ‰ä»»åŠ¡å°†å°è¯•ä¼˜é›…é€€å‡ºï¼‰")
    except Exception as e:
        logger.error(f"\nâŒ ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


def run():
    """
    ä¾¿æ·å¯åŠ¨å‡½æ•°ï¼ˆåŒæ­¥åŒ…è£…ï¼‰
    æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description="å¤šå¹³å°çˆ¬è™« Agentï¼ˆæ”¯æŒå°çº¢ä¹¦ã€Pinterestç­‰ï¼‰")
    parser.add_argument(
        "--concurrent",
        "-c",
        type=int,
        default=3,
        help="æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆé»˜è®¤3ï¼‰"
    )
    args = parser.parse_args()

    asyncio.run(main(max_concurrent=args.concurrent))


if __name__ == "__main__":
    run()
